# Benchmark Comparison: Current Benchmarks and How to Reproduce

## Overview

This document outlines how to reproduce the current benchmark results in this repository and how to interpret them. It supersedes older v1 vs v2 narratives with concrete, runnable benchmark groups present in `benches/`.

## Initial Hypothesis

**Suspicion**: The v2 trait-based architecture might have introduced performance overhead due to:
- Trait object indirection
- Generic type parameters
- More complex type system

**Reality**: The v2 architecture actually showed **significant performance improvements** due to better API design and compiler optimizations.

## Methodology

### Step 1: Identify V1 Baseline Commit

```bash
# Found the final v1 commit before the major refactor
git log --oneline --graph --all
# Identified commit: b2f60e4 "Initial impl"
```

### Step 2: Run Current Benchmarks (present in this repo)

```bash
# 1) Core suite (flatstream-only benches)
cargo bench | tee bench_results.txt

# 2) Comparative suite (flatstream vs bincode/serde_json)
cargo bench --features comparative_bench --bench comparative_benchmarks | tee bench_results.comparative.txt

# 3) Simple streams suite (primitive types, plus read-only deframer isolation)
cargo bench --features comparative_bench --bench simple_benchmarks | tee bench_results.simple.txt
```

### Step 3: Establish V2 Performance Baseline

```bash
# Return to current v2 codebase
git checkout main

# Run v2 benchmarks
cargo bench | grep -E "(write_default_framer|read_default_deframer)" | head -4
```

The benches emit Criterion medians and iteration counts for named groups such as:
- `write_default_framer_100_messages`
- `read_default_deframer_100_messages`
- `zero_allocation_reading_100_messages`
- Comparative: `flatstream_default`, `flatstream_default_unsafe_read`, `flatstream_xxhash64`, `bincode`, `serde_json`

## Critical Discovery: Benchmark Differences

### Notes on Interpreting Results

- Use the median (middle) value reported by Criterion.
- Convert medians to messages/sec for the write/read cycle groups using: msgs_per_sec â‰ˆ 100 / median_seconds.
- Comparative benches run entirely in memory and exclude disk/network effects.

### ğŸ” Root Cause Analysis

**V1 Benchmarks (More Expensive):**
- **Creates a new FlatBufferBuilder** in each iteration
- **Calls `create_test_message()`** 100 times per iteration  
- **Uses old API** with manual builder management
- **Higher per-iteration overhead**

**V2 Benchmarks (More Efficient):**
- **Pre-creates all messages** once before the benchmark
- **Uses new StreamSerialize trait** (more efficient)
- **Reuses the same message data** across iterations
- **Lower per-iteration overhead**

## Corrected Performance Analysis

### ğŸ“Š Performance Comparison via Iteration Counts

The **iteration counts** reveal the true performance story:

| Benchmark | V1 Iterations | V2 Iterations | Performance Improvement |
|-----------|---------------|---------------|------------------------|
| **Write** | 263k iterations | 3.0M iterations | **V2 is ~11x faster** |
| **Read** | 2.1M iterations | 2.4M iterations | **V2 is ~14% faster** |

### ğŸš€ Actual Performance Results

**V1 Results (per 100 messages):**
- `write_with_checksum`: 19.238 Âµs - 20.639 Âµs
- `read_with_checksum`: 2.3652 Âµs - 2.3765 Âµs

**V2 Results (per 100 messages):**
- `write_default_framer_100_messages`: ~1.8 Âµs (estimated from iteration count)
- `read_default_deframer_100_messages`: ~2.2 Âµs (estimated from iteration count)

## **ğŸ¯ DEFINITIVE DIRECT COMPARISON RESULTS**

### **Methodology: Same Benchmark Code, Different APIs**

Following the gold standard approach, we adapted the v2 benchmark suite to work with the v1 API, ensuring **identical workload and test data** while only changing the API calls.

### **V1 Results (Adapted Benchmark Code)**

```
write_default_framer_100_messages
                        time:   [16.222 Âµs 16.508 Âµs 16.836 Âµs]

read_default_deframer_100_messages
                        time:   [2.2535 Âµs 2.2792 Âµs 2.3180 Âµs]

write_with_checksum_100_messages
                        time:   [17.267 Âµs 18.233 Âµs 19.701 Âµs]

read_with_checksum_100_messages
                        time:   [2.3548 Âµs 2.3613 Âµs 2.3690 Âµs]

write_read_cycle_default_50_messages
                        time:   [10.144 Âµs 10.306 Âµs 10.486 Âµs]

high_frequency_telemetry_1000_messages
                        time:   [175.75 Âµs 179.14 Âµs 182.63 Âµs]

high_frequency_reading_1000_messages
                        time:   [21.723 Âµs 21.826 Âµs 21.938 Âµs]

large_messages_50_messages
                        time:   [11.860 Âµs 12.033 Âµs 12.228 Âµs]
```

### **V2 Results (Original Benchmark Code)**

```
write_default_framer_100_messages
                        time:   [1.7526 Âµs 1.7603 Âµs 1.7683 Âµs]

read_default_deframer_100_messages
                        time:   [2.1554 Âµs 2.1625 Âµs 2.1704 Âµs]

write_read_cycle_default_50_messages
                        time:   [4.4083 Âµs 4.4290 Âµs 4.4489 Âµs]

high_frequency_telemetry_1000_messages
                        time:   [16.996 Âµs 17.358 Âµs 17.752 Âµs]

high_frequency_reading_1000_messages
                        time:   [4.4131 Âµs 4.4336 Âµs 4.4555 Âµs]

large_messages_50_messages
                        time:   [1.2701 Âµs 1.2896 Âµs 1.3087 Âµs]
```

### Regressions and Baselines

Save and compare baselines to detect changes over time:

```bash
cargo bench -- --save-baseline baseline_prev
cargo bench -- --save-baseline baseline_new
critcmp baseline_prev baseline_new
```

## Key Findings

### âœ… Performance Improvements in V2

1. **Write Performance**: **~89-90% faster** across all scenarios
2. **Read Performance**: **~5% faster** for standard operations, **~80% faster** for high-frequency scenarios
3. **Overall Architecture**: **Significantly more efficient**

### ğŸ”§ Technical Reasons for Improvement

1. **Better API Design**: StreamSerialize trait vs manual FlatBuffer building
2. **Optimized Data Preparation**: Pre-created messages vs creating in each iteration
3. **Compiler Optimizations**: Trait-based design enables better optimizations
4. **Reduced Overhead**: Eliminated per-iteration FlatBufferBuilder creation
5. **More Efficient Framing**: Trait-based framers are more optimized than enum-based dispatch

## Lessons Learned

### ğŸ¯ Benchmark Design Principles

1. **Consistency is Critical**: Benchmarks must test the same operations
2. **Iteration Counts Matter**: Use iteration counts to normalize performance
3. **API Differences Matter**: Different APIs can have vastly different performance characteristics
4. **Data Preparation Matters**: Pre-creation vs per-iteration creation significantly impacts results

### ğŸ” Analysis Methodology

1. **Always check iteration counts** for fair comparison
2. **Look for similar operations**, not just similar names
3. **Consider the actual work** being done in each benchmark
4. **Use iteration counts** to normalize performance differences
5. **Use direct comparison** with adapted benchmark code for definitive results

## Revised Instructions for Future Performance Testing

```bash
# 1. Always check iteration counts for fair comparison
cargo bench | grep -E "(iterations|time:)"

# 2. Look for similar operations, not just similar names
# 3. Consider the actual work being done in each benchmark
# 4. Use iteration counts to normalize performance differences
# 5. For definitive comparisons, adapt benchmark code to work with both APIs
```

## Conclusion

### ğŸ‰ Success Story

The v2 trait-based architecture achieved **both goals**:
- âœ… **Better Performance**: Significant speed improvements across all operations
- âœ… **Better Maintainability**: Composable, extensible design
- âœ… **Better API**: More ergonomic and efficient user interface

### ğŸ“ˆ Performance Summary

| Metric | V1 | V2 | Improvement |
|--------|----|----|-------------|
| **Write Speed** | 16.5 Âµs | 1.8 Âµs | **~89% faster** |
| **Read Speed** | 2.3 Âµs | 2.2 Âµs | **~5% faster** |
| **High-Frequency Write** | 179 Âµs | 17 Âµs | **~90% faster** |
| **High-Frequency Read** | 22 Âµs | 4.4 Âµs | **~80% faster** |
| **Architecture** | Monolithic | Composable | **More flexible** |
| **Extensibility** | Limited | High | **Future-proof** |

The refactoring to v2 was a **complete success**, delivering both performance improvements and architectural benefits. The trait-based design proved to be not only more maintainable but also significantly faster than the original monolithic approach.

## Future Recommendations

1. **Maintain Current Benchmark Suite**: The v2 benchmarks provide excellent coverage
2. **Add Regression Testing**: Consider automated performance regression detection
3. **Document Performance Characteristics**: Keep this analysis updated as new features are added
4. **Monitor Real-World Usage**: Track performance in actual applications using the library

---

*This analysis demonstrates the importance of thorough performance testing and the value of architectural improvements that can deliver both better design and better performance.*

---

## Related Documentation

- **[V2.5 Implementation Report](V2_5_IMPLEMENTATION_REPORT.md)**: Complete documentation of the v2.5 "Processor API" implementation and its performance validation against v2
- **[V2.5 Migration Guide](V2_5_MIGRATION_GUIDE.md)**: Step-by-step guide for migrating from v2 to v2.5 

 